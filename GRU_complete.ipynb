{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz'\n",
    "\n",
    "def getunzipped(theurl):\n",
    "    name = './dbpedia_csv.tar.gz'\n",
    "    try:\n",
    "        name, hdrs = urllib.request.urlretrieve(theurl, name)\n",
    "    except IOError as e:\n",
    "        print(\"Can't retrieve %r to %r: %s\" % (theurl, thedir, e))\n",
    "        return\n",
    "    try:\n",
    "        z = tarfile.open(name, \"r:gz\")\n",
    "        z.extractall()\n",
    "        z.close()\n",
    "    except tarfile.error as e:\n",
    "        print(\"Bad zipfile (from %r): %s\" % (theurl, e))\n",
    "        return\n",
    "\n",
    "    print(\"Data Downloaded and unzipped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Downloaded and unzipped!\n"
     ]
    }
   ],
   "source": [
    "getunzipped(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E. D. Abbott Ltd</td>\n",
       "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwan-Stabilo</td>\n",
       "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Q-workshop</td>\n",
       "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Marvell Software Solutions Israel</td>\n",
       "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bergan Mercy Medical Center</td>\n",
       "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                              title  \\\n",
       "0      1                   E. D. Abbott Ltd   \n",
       "1      1                     Schwan-Stabilo   \n",
       "2      1                         Q-workshop   \n",
       "3      1  Marvell Software Solutions Israel   \n",
       "4      1        Bergan Mercy Medical Center   \n",
       "\n",
       "                                                text  \n",
       "0   Abbott of Farnham E D Abbott Limited was a Br...  \n",
       "1   Schwan-STABILO is a German maker of pens for ...  \n",
       "2   Q-workshop is a Polish company located in Poz...  \n",
       "3   Marvell Software Solutions Israel known as RA...  \n",
       "4   Bergan Mercy Medical Center is a hospital loc...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('dbpedia_csv/train.csv',header=None,names=['class','title','text'], nrows=1500)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TY KU</td>\n",
       "      <td>TY KU /taɪkuː/ is an American alcoholic bever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Odd Lot Entertainment</td>\n",
       "      <td>OddLot Entertainment founded in 2001 by longt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Henkel</td>\n",
       "      <td>Henkel AG &amp; Company KGaA operates worldwide w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>GOAT Store</td>\n",
       "      <td>The GOAT Store (Games Of All Type Store) LLC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RagWing Aircraft Designs</td>\n",
       "      <td>RagWing Aircraft Designs (also called the Rag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                     title  \\\n",
       "0      1                     TY KU   \n",
       "1      1     Odd Lot Entertainment   \n",
       "2      1                    Henkel   \n",
       "3      1                GOAT Store   \n",
       "4      1  RagWing Aircraft Designs   \n",
       "\n",
       "                                                text  \n",
       "0   TY KU /taɪkuː/ is an American alcoholic bever...  \n",
       "1   OddLot Entertainment founded in 2001 by longt...  \n",
       "2   Henkel AG & Company KGaA operates worldwide w...  \n",
       "3   The GOAT Store (Games Of All Type Store) LLC ...  \n",
       "4   RagWing Aircraft Designs (also called the Rag...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('dbpedia_csv/test.csv',header=None,names=['class','title','text'], nrows=1500)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train['text'].get_values()\n",
    "X_test = test['text'].get_values()\n",
    "y_train = train['class'].get_values()\n",
    "y_test= train['class'].get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_doc_length = 50\n",
    "\n",
    "train_lengths = train['text'].apply(lambda x: min(max_doc_length, len(x.split(' '))))\n",
    "test_lengths = test['text'].apply(lambda x: min(max_doc_length, len(x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_lengths = train_lengths.values\n",
    "test_lengths = test_lengths.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VocabularyProcessor \n",
    "Has an sklearn type interface - instantiate, call .fit() and .transform()\n",
    "\n",
    "Creates a mapping between words and ID's.\n",
    "\n",
    "Fit creates the mapping word-> ID, and transform replaces the words with relevant IDS.\n",
    "\n",
    "We can then use tf.nn.embedding_lookup(...) to look up the vector representations that correspond to each ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_doc_length, min_frequency=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_transformed = np.array(list(preprocessor.fit_transform(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have replaced each word with a numerical ID corresponding to a word in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    4,    0, ...,    0,    0,    0],\n",
       "       [   0,    5,    6, ...,    0,    0,    0],\n",
       "       [   0,    5,    6, ...,   11,    0,    0],\n",
       "       ..., \n",
       "       [2263, 1804,   54, ...,  946,  630,  279],\n",
       "       [   0,  425,    0, ...,    0,    0,    0],\n",
       "       [3196,  397,    5, ...,  104,   75,    2]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'the' corresponds to ID: 3\n",
      "ID 25 corresponds to: which\n"
     ]
    }
   ],
   "source": [
    "print(\"The word 'the' corresponds to ID:\",preprocessor.vocabulary_.get('the')) #Get the ID for a word\n",
    "print(\"ID 25 corresponds to:\",preprocessor.vocabulary_.reverse(25)) #Get the word that corresponds to an ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_length = len(preprocessor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_transformed = np.array(list(preprocessor.transform(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    5,   14,   28, 3332, 1057,    7,   21,  242,\n",
       "           2,    0,    1,   64,    0,    8,    0,    7,    9,   19,    2,\n",
       "         240,    1,    5,   39,    2,   36,   86,   77,   36,   86, 2376,\n",
       "          18,    2,   36,   86,    0,    0,  461,    0,   33,  414,    2,\n",
       "         248,   97,    6,  526,  333,   15],\n",
       "       [   0,    0,  169,   19,    2,  324,   11,    0, 3616,    0,    0,\n",
       "           1,    0, 2164,    0,    8,    0,    0,    5,    6,  136,   78,\n",
       "           1,    0,    7,   18,    2,    0,   77,   71,    0,    0,  236,\n",
       "           3,  136,    0,    4,    0, 1036,    0,    0, 1972,    0, 1134,\n",
       "          80,  136,    0,    4,  478, 1972]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transformed[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<UNK> of <UNK> E D <UNK> Limited was a British <UNK> business based in <UNK> Surrey trading under that name from 1929 A major part of their output was under <UNK> to motor vehicle manufacturers Their business closed in 1972 <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(preprocessor.reverse(X_train_transformed[0,None].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now our data is prepared we can start modelling..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basics have been implemented, but you need to implement the RNN functionality, and define the loss function.\n",
    "\n",
    "As a starting point, you several comments have been provided in the relevant name scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "embedding_size=100\n",
    "rnn_cell_size = 100\n",
    "n_classes=19\n",
    "learning_rate = 0.001\n",
    "\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('placeholders'):\n",
    "        tf_X = tf.placeholder(shape=[None, max_doc_length], dtype=tf.int32) #This is a matrix of word IDs\n",
    "        tf_y = tf.placeholder(shape=[None,1], dtype=tf.int64) #This is a list of class labels e.g. 1,5,3 etc.\n",
    "        tf_lengths = tf.placeholder(shape=[None,], dtype=tf.int32) #This is the lengths \n",
    "                                                                    #of each document in sample\n",
    "        \n",
    "    with tf.name_scope('variables'):\n",
    "        embedding_matrix = tf.Variable(tf.random_uniform(shape=[vocab_length, embedding_size],\n",
    "                                                         dtype=tf.float32))\n",
    "        W = tf.Variable(tf.truncated_normal(shape=[rnn_cell_size, n_classes ]))\n",
    "        b = tf.Variable(tf.random_uniform(shape=[n_classes,]))\n",
    "        tf_y_one_hot = tf.one_hot(tf_y,n_classes)\n",
    "    \n",
    "    with tf.name_scope('embbeding_lookup'):\n",
    "        #You need to look up the word ID and get replace with the associated word vector.\n",
    "        #Verify the size is (?, max_doc_length, embedding_size)\n",
    "        tf_X_embedded = tf.nn.embedding_lookup(embedding_matrix, tf_X) \n",
    "        \n",
    "    with tf.name_scope('run_rnn'):\n",
    "        #You need to define a cell type - use GRUCell\n",
    "        #Also need to feed data into the RNN and get the final_state - use dynamic_rnn\n",
    "        cell = tf.contrib.rnn.GRUCell(rnn_cell_size)\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell,tf_X_embedded, tf_lengths, dtype=tf.float32)\n",
    "        \n",
    "    with tf.name_scope('output_layer'):\n",
    "        #Create a variable logits which is final_state * W + b\n",
    "        #Pass this through a softmax to create prediction probabilities.\n",
    "        logits = tf.matmul(final_state,W)+b\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        #Add a loss and an optimizer\n",
    "        loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_y_one_hot))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "    with tf.name_scope('validation'):\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf_y,tf.argmax(predictions,1)), tf.float32))\n",
    "        \n",
    "    with tf.name_scope('init'):\n",
    "        init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------- step 0 ----------------------------------------\n",
      "[1.0]\n",
      "---------------------------------------- step 100 ----------------------------------------\n",
      "[1.0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-5f87c926a2e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                      \u001b[0mtf_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                      tf_lengths: train_lengths[ind] }\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mprint_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chris/anaconda2/envs/p3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chris/anaconda2/envs/p3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chris/anaconda2/envs/p3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/chris/anaconda2/envs/p3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chris/anaconda2/envs/p3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "print_step = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    number_of_steps = 500\n",
    "    train_indicies = np.arange(X_train_transformed.shape[0])\n",
    "    test_indicies = np.arange(X_test_transformed.shape[0])\n",
    "    \n",
    "    session.run(init_op)\n",
    "    \n",
    "    for step in range(number_of_steps):\n",
    "        ind = np.random.choice(train_indicies, size=batch_size, replace=False)\n",
    "        feed_dict = {tf_X:X_train_transformed[ind], \n",
    "                     tf_y:y_train[ind,None], \n",
    "                     tf_lengths: train_lengths[ind] }\n",
    "        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        \n",
    "        if step%print_step == 0:\n",
    "            ind = np.random.choice(test_indicies, size=batch_size, replace=False)\n",
    "            feed_dict = {tf_X:X_test_transformed[ind], \n",
    "                     tf_y:y_test[ind,None], \n",
    "                     tf_lengths: test_lengths[ind] }\n",
    "            a = session.run([accuracy], feed_dict=feed_dict)\n",
    "            print('-'*40, \"step \"+str(step), '-'*40)\n",
    "            print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:p3]",
   "language": "python",
   "name": "conda-env-p3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
