{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url = 'https://github.com/le-scientifique/torchDatasets/raw/master/dbpedia_csv.tar.gz'\n",
    "\n",
    "def getunzipped(theurl):\n",
    "    name = './dbpedia_csv.tar.gz'\n",
    "    try:\n",
    "        name, hdrs = urllib.request.urlretrieve(theurl, name)\n",
    "    except IOError as e:\n",
    "        print(\"Can't retrieve %r to %r: %s\" % (theurl, thedir, e))\n",
    "        return\n",
    "    try:\n",
    "        z = tarfile.open(name, \"r:gz\")\n",
    "        z.extractall()\n",
    "        z.close()\n",
    "    except tarfile.error as e:\n",
    "        print(\"Bad zipfile (from %r): %s\" % (theurl, e))\n",
    "        return\n",
    "\n",
    "    print(\"Data Downloaded and unzipped!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Downloaded and unzipped!\n"
     ]
    }
   ],
   "source": [
    "getunzipped(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>E. D. Abbott Ltd</td>\n",
       "      <td>Abbott of Farnham E D Abbott Limited was a Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Schwan-Stabilo</td>\n",
       "      <td>Schwan-STABILO is a German maker of pens for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Q-workshop</td>\n",
       "      <td>Q-workshop is a Polish company located in Poz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Marvell Software Solutions Israel</td>\n",
       "      <td>Marvell Software Solutions Israel known as RA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Bergan Mercy Medical Center</td>\n",
       "      <td>Bergan Mercy Medical Center is a hospital loc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                              title  \\\n",
       "0      1                   E. D. Abbott Ltd   \n",
       "1      1                     Schwan-Stabilo   \n",
       "2      1                         Q-workshop   \n",
       "3      1  Marvell Software Solutions Israel   \n",
       "4      1        Bergan Mercy Medical Center   \n",
       "\n",
       "                                                text  \n",
       "0   Abbott of Farnham E D Abbott Limited was a Br...  \n",
       "1   Schwan-STABILO is a German maker of pens for ...  \n",
       "2   Q-workshop is a Polish company located in Poz...  \n",
       "3   Marvell Software Solutions Israel known as RA...  \n",
       "4   Bergan Mercy Medical Center is a hospital loc...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('dbpedia_csv/train.csv',header=None,names=['class','title','text'])\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TY KU</td>\n",
       "      <td>TY KU /taɪkuː/ is an American alcoholic bever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Odd Lot Entertainment</td>\n",
       "      <td>OddLot Entertainment founded in 2001 by longt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Henkel</td>\n",
       "      <td>Henkel AG &amp; Company KGaA operates worldwide w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>GOAT Store</td>\n",
       "      <td>The GOAT Store (Games Of All Type Store) LLC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RagWing Aircraft Designs</td>\n",
       "      <td>RagWing Aircraft Designs (also called the Rag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                     title  \\\n",
       "0      1                     TY KU   \n",
       "1      1     Odd Lot Entertainment   \n",
       "2      1                    Henkel   \n",
       "3      1                GOAT Store   \n",
       "4      1  RagWing Aircraft Designs   \n",
       "\n",
       "                                                text  \n",
       "0   TY KU /taɪkuː/ is an American alcoholic bever...  \n",
       "1   OddLot Entertainment founded in 2001 by longt...  \n",
       "2   Henkel AG & Company KGaA operates worldwide w...  \n",
       "3   The GOAT Store (Games Of All Type Store) LLC ...  \n",
       "4   RagWing Aircraft Designs (also called the Rag...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('dbpedia_csv/test.csv',header=None,names=['class','title','text'])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = train['text'].get_values()\n",
    "X_test = test['text'].get_values()\n",
    "y_train = train['class'].get_values()\n",
    "y_test= test['class'].get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_doc_length = 50\n",
    "\n",
    "train_lengths = train['text'].apply(lambda x: min(max_doc_length, len(x.split(' '))))\n",
    "test_lengths = test['text'].apply(lambda x: min(max_doc_length, len(x.split(' '))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_lengths = train_lengths.values\n",
    "test_lengths = test_lengths.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VocabularyProcessor \n",
    "Has an sklearn type interface - instantiate, call .fit() and .transform()\n",
    "\n",
    "Creates a mapping between words and ID's.\n",
    "\n",
    "Fit creates the mapping word-> ID, and transform replaces the words with relevant IDS.\n",
    "\n",
    "We can then use tf.nn.embedding_lookup(...) to look up the vector representations that correspond to each ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocessor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_doc_length, min_frequency=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_transformed = np.array(list(preprocessor.fit_transform(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have replaced each word with a numerical ID corresponding to a word in our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8857,      3,  17539, ...,      0,      0,      0],\n",
       "       [     0,      4,      5, ...,      0,      0,      0],\n",
       "       [     0,      4,      5, ...,      9, 114800,      0],\n",
       "       ..., \n",
       "       [     8,      0,   6291, ...,      0,      0,      0],\n",
       "       [     0,  71320,   4408, ...,      3,    686,      0],\n",
       "       [ 12952,   9652,  53503, ...,  14682,    595,  12709]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'the' corresponds to ID: 1\n",
      "ID 25 corresponds to: known\n"
     ]
    }
   ],
   "source": [
    "print(\"The word 'the' corresponds to ID:\",preprocessor.vocabulary_.get('the')) #Get the ID for a word\n",
    "print(\"ID 25 corresponds to:\",preprocessor.vocabulary_.reverse(25)) #Get the word that corresponds to an ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_length = len(preprocessor.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_transformed = np.array(list(preprocessor.transform(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     0,  33095,      0,      4,     15,     34,  13638,  12026,\n",
       "            58,     32,   2092,      2,  34220,      6,    154,  19432,\n",
       "             8,      0,     58,      7,    108,      2,    254,      6,\n",
       "             4,    672,      2,     46,    111,    133,     46,    111,\n",
       "          1789,     69,      2,     46,    111,      0,  33095,   1012,\n",
       "         12873,     40,    218,      2,    342,    137,      5,   2554,\n",
       "          2603,     17],\n",
       "       [  7874,  13438,   1130,    108,      2,    346,      9,   5924,\n",
       "          3640,  26563,  53628,      6,   9446,   4389, 115134,      8,\n",
       "         10402, 102537,      4,      5,     22,    454,      6,  11213,\n",
       "            58,     69,      2,  14655,    133,    144,   7874,  13438,\n",
       "           138,      1,     22,    579,      3,   9822,   1198,  38006,\n",
       "         13657,     82,  27928,   2842,     56,     22,    579,      3,\n",
       "           155,     82]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transformed[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929 A major part of their output was under sub-contract to motor vehicle manufacturers Their business closed in 1972 <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(preprocessor.reverse(X_train_transformed[0,None].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now our data is prepared we can start modelling..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basics have been implemented, but you need to implement the RNN functionality, and define the loss function.\n",
    "\n",
    "As a starting point, you several comments have been provided in the relevant name scopes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "embedding_size=100\n",
    "rnn_cell_size = 100\n",
    "n_classes=15\n",
    "learning_rate = 0.005\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "    with tf.name_scope('placeholders'):\n",
    "        tf_X = tf.placeholder(shape=[None, max_doc_length], dtype=tf.int32) #This is a matrix of word IDs\n",
    "        tf_y = tf.placeholder(shape=[None], dtype=tf.int64) #This is a list of class labels e.g. 1,5,3 etc.\n",
    "        tf_lengths = tf.placeholder(shape=[None,], dtype=tf.int32) #This is the lengths \n",
    "                                                                    #of each document in sample\n",
    "        \n",
    "    with tf.name_scope('variables'):\n",
    "        embeddings = tf.Variable(tf.random_uniform([vocab_length,embedding_size],-1.0,1.0)) \n",
    "        W = tf.Variable(tf.truncated_normal(shape=[rnn_cell_size, n_classes ]))\n",
    "        b = tf.Variable(tf.random_uniform(shape=[n_classes,]))\n",
    "        tf_y_one_hot = tf.one_hot(tf_y,n_classes)\n",
    "    \n",
    "    with tf.name_scope('embbeding_lookup'):\n",
    "        #You need to look up the word ID and get replace with the associated word vector.\n",
    "        #Verify the size is (?, max_doc_length, embedding_size)\n",
    "        tf_X_embedded = tf.nn.embedding_lookup(embeddings, tf_X) \n",
    "        \n",
    "    with tf.name_scope('run_rnn'):\n",
    "        #You need to define a cell type - use GRUCell\n",
    "        #Also need to feed data into the RNN and get the final_state - use dynamic_rnn\n",
    "        cell = tf.contrib.rnn.BasicRNNCell(rnn_cell_size)\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell,tf_X_embedded, tf_lengths, dtype=tf.float32)\n",
    "        \n",
    "    with tf.name_scope('attention'):\n",
    "        #The most complex part of this is the multiplication - TF doesn't have a nice way to handle 3d by 2d.\n",
    "        \n",
    "        #attention Variables\n",
    "        W_a1 = tf.Variable(tf.random_uniform(shape=[rnn_cell_size,1]))\n",
    "        b_a1 = tf.Variable(tf.random_uniform(shape=[1,]))\n",
    "        W_a2 = tf.Variable(tf.random_uniform(shape=[max_doc_length,max_doc_length]))\n",
    "        b_a2 = tf.Variable(tf.random_uniform(shape=[max_doc_length,]))\n",
    "        \n",
    "        #Basicly a 1 hidden layer MLP to compute attention\n",
    "        reshape_out= tf.reshape(outputs, [-1, rnn_cell_size])\n",
    "        wo = tf.matmul(reshape_out, W_a1)\n",
    "        attn_hidden = tf.nn.tanh(tf.reshape(wo, [-1, max_doc_length])+b_a1)\n",
    "        attn_output = tf.nn.softmax(tf.matmul(attn_hidden, W_a2)+b_a2)\n",
    "        a = tf.expand_dims(attn_output,-1)\n",
    "        \n",
    "        #weighted average state using attention\n",
    "        attn_state = tf.reduce_sum(tf.multiply(a, outputs), axis=1)\n",
    "        \n",
    "        \n",
    "    with tf.name_scope('output_layer'):\n",
    "        #Create a variable logits which is final_state * W + b\n",
    "        #Pass this through a softmax to create prediction probabilities.\n",
    "        logits = tf.matmul(attn_state,W)+b\n",
    "        predictions=tf.argmax(tf.nn.softmax(logits),1)\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        #Add a loss and an optimizer\n",
    "        loss= tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_y_one_hot))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "        \n",
    "    with tf.name_scope('validation'):\n",
    "        correct_prediction = tf.equal(tf_y, predictions)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "        \n",
    "    with tf.name_scope('init'):\n",
    "        init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                          | 0/301 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.0234767452876\n",
      "accuracy 0.0797266\n",
      "---------------------------------------- step 0 ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████▊                                        | 150/301 [00:52<00:47,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 1.39103110154\n",
      "accuracy 0.840586\n",
      "---------------------------------------- step 150 ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████▋| 300/301 [01:44<00:00,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average loss: 0.367573136389\n",
      "accuracy 0.921523\n",
      "---------------------------------------- step 300 ----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 301/301 [01:50<00:00,  1.89s/it]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "print_step = 150\n",
    "from tqdm import tqdm\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    number_of_steps = 301\n",
    "    train_indicies = np.arange(X_train_transformed.shape[0])\n",
    "    test_indicies = np.arange(X_test_transformed.shape[0])\n",
    "    \n",
    "    session.run(init_op)\n",
    "    average_loss = 0\n",
    "    \n",
    "    for step in tqdm(range(number_of_steps)):\n",
    "        ind =  np.random.choice(np.arange(X_train_transformed.shape[0]), batch_size)\n",
    "        feed_dict = {tf_X:X_train_transformed[ind], \n",
    "                     tf_y:y_train[ind], \n",
    "                     tf_lengths: train_lengths[ind] }\n",
    "        l,_ = session.run([loss,optimizer ], feed_dict=feed_dict)\n",
    "        average_loss+=l\n",
    "        \n",
    "        if step%print_step == 0:\n",
    "            print (\"average loss:\",average_loss/print_step)\n",
    "            ind2 =  np.random.choice(np.arange(X_test_transformed.shape[0]), batch_size*200)\n",
    "            test_dict = {tf_X:X_test_transformed[ind2], \n",
    "                     tf_y:y_test[ind2], \n",
    "                     tf_lengths: test_lengths[ind2] }\n",
    "            print(\"accuracy\",accuracy.eval(feed_dict=test_dict))\n",
    "            print('-'*40, \"step \"+str(step), '-'*40)\n",
    "            \n",
    "            average_loss=0\n",
    "    test_dict = {tf_X:X_test_transformed[5:30], \n",
    "                     tf_y:y_test[5:30], \n",
    "                     tf_lengths: test_lengths[5:30] }\n",
    "    attn = a.eval(feed_dict=test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc = 11\n",
    "phrase = next(preprocessor.reverse(X_test_transformed[doc,None].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45, 27, 38, 35, 42, 46, 40, 23, 31, 24,  1, 48, 19, 43, 49,  2, 22,\n",
       "        4, 36, 14, 30, 32, 25, 44, 37, 20, 47, 26, 15, 34, 16, 41,  0,  3,\n",
       "       33, 21, 18, 39,  7, 17,  5, 12, 28, 29,  8, 10,  6, 13,  9, 11], dtype=int64)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(attn[doc].ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Walter Aircraft Engines was a company <b>that</b> manufactured aircraft <b>engines</b> <b>particularly</b> <b>the</b> \t&#60;UNK> <b>turboprop</b> Based in Prague Czech Republic the company has been a subsidiary of GE Aviation since July 2008 \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> \t&#60;UNK> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "out= ''\n",
    "for i, word in enumerate(X_test_transformed[doc]):\n",
    "    r_word = preprocessor.vocabulary_.reverse(word)\n",
    "    if r_word == '<UNK>':\n",
    "        r_word = '\t&#60;UNK>'\n",
    "    if i in np.argsort(attn[doc].ravel())[-5:]:\n",
    "        out += '<b>'+ r_word + '</b> '\n",
    "    else:\n",
    "        out += r_word + ' '\n",
    "HTML(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tfgpu]",
   "language": "python",
   "name": "conda-env-tfgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
